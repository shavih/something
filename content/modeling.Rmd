---
title: 'Project 2: Modeling'
author: "Shavi Hewage, SH42727"
date: "11/24/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sandwich)
library(lmtest)
library(ggplot2)
library(plotROC) 
library(MASS)
library(glmnet)
library(mvtnorm)
library(ggExtra)
```

##Introduction
This dataset is a fun amalgmation of a bunch of different state-level health data from 2017. We're going to use this to look at differences between Democratic and Republican states in terms of their healthcare, and then go the other way and predict which states are Democrat or Republican affiliated based on their healthcare statistics.   
Variables include:  
State, affiliation during 2016 election ([source](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PEJ5QU)), population ([source](https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html)), antibiotic prescription rate per thousand ([source](https://www.cdc.gov/antibiotic-use/community/programs-measurement/state-local-activities/outpatient-antibiotic-prescriptions-US-2017.html)), opioid prescription rate per thousand ([source](https://www.cdc.gov/drugoverdose/maps/rxstate2017.html)), total payments from drug companies accepted by doctors per state ([source](https://www.cms.gov/OpenPayments/Explore-the-Data/Dataset-Downloads)), drug deaths per 100,000 population ([source](https://www.americashealthrankings.org/explore/annual/measure/Drugdeaths/state/ALL?edition-year=2017)), and public health funding per person ([source](https://www.americashealthrankings.org/explore/annual/measure/PH_funding/state/ALL?edition-year=2017)). 

##PART I: MANOVA  
First, we're gonna do a MANOVA to see if numeric variables have a mean difference across categorical variables.
```{r}
datafinal<-read.csv("/Users/shavihewage/datafinal")

mandata<-manova(cbind(APPT,OPPT,PPF,ddpht,total_payment)~presidential, data=datafinal)
summary(mandata)
summary.aov(mandata)

pairwise.t.test(datafinal$APPT, datafinal$presidential, p.adj="none")
pairwise.t.test(datafinal$OPPT, datafinal$presidential, p.adj="none")

```
A MANOVA was run to determine the effect of state affiliation on five dependent variables. Of those five variables, two were significant. Pairwise t-tests were run on antibiotic prescription rate and opioid prescription rate, and there was a significant difference in the antibiotic and opioid prescription rates between democratic and republican affiliated states.  
There were eight tests run, which means that the probability of a type I error is 0.4. Therefore, the $\alpha$ needs to be adjusted. this would be $\alpha = 0.05/8 = 0.00625$. Even with the correction, however, the difference in prescription rates for both drug categories is still significant.  

####_Assumptions_  
Okay, take a deep breath. We're going to talk about MANOVA assumptions. No, wait! Please don't leave. Sit down. It's going to be okay.  
MANOVA assumes that the data is randomly sampled with independent observations. It also assumes mutlivariate normality of dependant variables.
```{r}
ggplot(datafinal, aes(x = APPT, y = OPPT)) +
geom_point(alpha = .5) + geom_density_2d(h=2) + coord_fixed() + facet_wrap(~presidential)

```
Hmm! Fun! It doesn't look multivariately normal. Looks like the MANOVA is impossible to please. Well, it's time to move on.  
  
##PART II: Randomization Tests  
Since the difference that appeared significant in the MANOVA part was opioid and antibiotic prescriptions by state, this randomization test will be conducted on the rates of antibiotic prescriptions per thousand.  
  
####_Null and Alternate Hypotheses_  
$H_{0}=$ The opioid prescription rate per thousand is the same for democratic versus republican states.  
$H_{A}=$ The opioid prescription rate per thousand is different for democratic versus republican states.

```{r}
#Original mean between the two groups
mean(datafinal[datafinal$presidential=="R",]$APPT)-
  mean(datafinal[datafinal$presidential=="D",]$APPT)

#Randomization test
r_dist <- vector()
for(i in 1:5000){
  new<-data.frame(APPT=sample(datafinal$APPT), party=datafinal$presidential)
  r_dist[i]<-mean(new[new$party=="R",]$APPT)-
    mean(new[new$party=="D",]$APPT)}

#P-value
mean(r_dist>173.56)*2
```
  
The randomization test produced a p-value of 0, which makes sense because the p-value that was produced by pairwise t-tests in the prior step was also very low. This p-value tells us that there is a significant difference in the antibiotic prescription rates between democratic-affiliated states and republican-affiliated states.   
  
  
####_Visualizing null and test statistic_  
```{r}
{hist(r_dist, main="Randomized Distribution of Mean Differences", ylab="", 
      xlab="Difference Between Democrat and Republican States"); 
  abline(v=173.56, col="red")}
```

We can see in the distribution of the differences in mean that the original difference in mean lies far to the right, indicating that there is a low likelihood of there being no difference in mean between the two groups.    
  
  
##PART III: Linear Regression Model  
Now it's time to predict a response variable, antibiotic prescriptions per 1000, from two other variables, political affiliation of the state and the total amount of payment accepted by state, per person. This is going to be extremely fun. The payment per person variable was made using mutate, by dividing the total payment per state by the population of the state. All the numeric variables were mean centered.  

```{r}
datafinal <- datafinal %>% mutate(perperson=total_payment/population)

datafinal$perperson_c <- datafinal$perperson-mean(datafinal$perperson)
datafinal$APPT_c <- datafinal$APPT-mean(datafinal$APPT)

part3 <- lm(APPT_c ~ perperson_c*presidential, data=datafinal)
summary(part3)
```
  
Looking at the coefficients, for Republican states, it appears that for every 1 increase in antibiotic prescriptions per thousand people, the amount of money obtained per person by doctors in that state increases by $9.775 relative to the mean. Furthermore, the number of antibiotic prescriptions per thousand population decreases by 195.152 relative to the mean when the state is Democratic. With regard to the interaction, for states that are Democratic, the number of antibiotic prescriptions per 1000 people decreases by 6.270 for every 1.00 dollar increase in big pharma money per person.  

```{r}
datafinal$presidential<-factor(datafinal$presidential,levels=c("R","D"),labels=c("R","D"))

ggplot(datafinal, aes(x=perperson, y=APPT))+geom_point(aes(color=presidential))+
  geom_smooth(method="lm", se=F, fullrange=T, aes(color=presidential))+
  ggtitle("Antibiotic Prescriptions Per Thousand vs Payments Accepted Per Person")+ 
xlab("Payments Per Person")+ylab("Antibiotic Prescription Per 1000 Population")+theme_classic()
```
  
####_Assumptions_    
The assumptions for this are a linear relationship between the predictor and response variable, independent observations, random sampling, normally distributed residuals, and equal variances.    


```{r}
resids<-part3$residuals

#Tests for normality
shapiro.test(resids)
#Tests for homoskedasticity
bptest(part3)
```
  
The data appears to be roughly linear (see graph).
The null hypothesis for the Shapiro-Wilk test is that the distribution is normal. Because the p-value is above 0.05, we fail to reject the null hypothesis that the distribution is normal.  
The null hypothesis for the Breusch-Pagan test is that the data is homoskedastic. The p-value for this test was 0.067, which is higher than the $\alpha$ of 0.05, so we fail to reject the null hypothesis that the data is homoskedastic.  
Therefore, the assumptions are met for this test.      
    
####_Regression with Robust Standard Errors_    
This is the same model run with heteroskedasticity robust standard errors.  
```{r}
coeftest(part3, vcov = vcovHC(part3))

summary(part3)$r.sq
```
    
When computing the same regression with robust standard errors, the standard errors (and subsequently the p-values) of the model were slightly lower than without the utilization robust standard errors. The same variable that was significant in the original model, political alignment during the 2016 election, was significant in the robust model.     
The proportion of variance explained by the model is 0.241.    
  
####_Same Model, No Interaction_  
```{r}
part3_2 <- lm(APPT_c ~ perperson_c+presidential, data=datafinal)
summary(part3_2)
```
This model includes only main effects and no interaction. For this, the numbers change slightly, but the p-values are still relatively the same. For republican states, for every 1 increase in antibiotic prescriptions per thousandpeople, the amount of money obtained per person by doctors in that state increases by $4.552 from the mean. Furthermore, controlling for payments obtained per person, the number of antibiotic prescriptions per thousand population decreases by 189.71 relative to the mean when the state is Democratic.    
  
##PART IV: Bootstrapped Standard Errors       
Here we are computing the bootstrapped standard errors for the same model as part III.    
  
```{r}
sample_dist<-replicate(5000, {
  bootstrapped<-datafinal[sample(nrow(datafinal), replace=TRUE),]
  part4<-lm(APPT_c ~ perperson_c*presidential, data=bootstrapped)
  coef(part4)
})

sample_dist%>%t%>%as.data.frame%>%summarize_all(sd)

```
As we can see here, the standard errors when doing the bootstrapped sample were slightly lower than the original standard error and the robust standard errors.
  
##PART V: Logistic Regression    
Now we're going to predict a binary (whether or not a Democratic state)   
```{r}
part5<-glm(presidential~APPT+perperson+OPPT+ddpht, data=datafinal, family=binomial(link="logit"))
exp(coeftest(part5))
```
  
For every 1 increase in antibiotic prescriptions per thousand population, the odds of the state being Republican increases by 0.99. For every $1 increase in funding obtained by doctors per person, the odds of the state being Republican increases by 1.24. For every 1 increase in opioid prescriptions per thousand population, the odds of the state being Republican increases by 0.98. Finally, for every 1 increase in drug deaths per hundred thousand population, the odds of being a Republican state increases by 1.065.


```{r}
#Probability of being Democrat-affiliated state
datafinal$prob<-predict(part5,type="response") 
datafinal$pred<-ifelse(datafinal$prob>0.5,"Democrat","Republican")

#Confusion Matrix
table(prediction=as.numeric(datafinal$prob>.5),truth=datafinal$presidential)%>%addmargins

#Accuracy
(25+17)/51
#Sensitivity
17/21
#Specificity
25/30
#Recall
17/22
```
The accuracy of the model is 0.823. The sensitivty, or true positive rate (the number of Democrat states that are actually Democrat states) is 0.809. The specificity, or true negative rate (the number of Republican states that are actually republican states) is 0.833. The precision is 0.772.

####_Density Plot_   
```{r}
datafinal$logpred<-predict(part5, type="link")
datafinal%>%ggplot()+geom_density(aes(logpred,color=presidential,fill=presidential), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+ggtitle("Density Plot of predicted Republican vs. Democrat")+xlab("predictor (logit)")
```
  
####_ROC Plot_  
```{r}
datafinal <- datafinal %>% mutate(y=as.numeric(presidential)-1)
ROC<-ggplot(data=datafinal)+geom_roc(aes(d=y,m=prob),n.cuts=0)
ROC
```
  
Very interesting! Looks like our model is doing okay! What's the AUC, you ask? This is being calculated with the `plotROC` package.    
```{r}
calc_auc(ROC)
```
It's 0.906! Not bad! The AUC is the probability that a random state selected that is Democratic has a higher prediction of being Democratic than a random state selected that is Republican. This means our model is pretty good at predicting overall.  

####_Cross Validation_  
But let's not get ahead of ourselves--it's time to see how our model performs out of sample. First, here's that diags function from class that I'm going to use later to get the accuracy, recall, and sensitivity. 
```{r}
## GIVE IT PREDICTED PROBS AND TRUTH LABELS, RETURNS VARIOUS DIAGNOSTICS
class_diag<-function(probs,truth){
 tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
 acc=sum(diag(tab))/sum(tab)
 sens=tab[2,2]/colSums(tab)[2]
 spec=tab[1,1]/colSums(tab)[1]
 ppv=tab[2,2]/rowSums(tab)[2]
 if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
 #CALCULATE EXACT AUC
 ord<-order(probs, decreasing=TRUE)
 probs <- probs[ord]; truth <- truth[ord]
 TPR=cumsum(truth)/max(1,sum(truth))
 FPR=cumsum(!truth)/max(1,sum(!truth))
 dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
 TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
 n <- length(TPR)
 auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
 data.frame(acc,sens,spec,ppv,auc)
} 
```

Now we're doing k-fold CV:  
```{r}
set.seed(1234)
k=10 

data1<-datafinal[sample(nrow(datafinal)),]
folds<-cut(seq(1:nrow(datafinal)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
 train<-data1[folds!=i,]
 test<-data1[folds==i,]
 truth<-test$presidential
 fit<-glm(presidential~APPT+perperson+OPPT+ddpht, data=train, family=binomial(link="logit"))
 probs<-predict(fit,newdata = test,type="response")
 diags<-rbind(diags,class_diag(probs,truth))
}

apply(diags,2,mean)
```
  
The average out-of-sample accuracy is 0.80, the sensitivity is 0.816, and the recall is 0.833. Compared to the model on its own, it is performing slightly worse, the AUC has reduced to 0.85 compared to the 0.906 from before.
  
##PART VI: LASSO Regression
choose a variable and then run regression lasso with all other variables as predictors
```{r}
mat<-model.matrix(part5)
y<-as.matrix(datafinal$y)
x<-as.data.frame(mat) %>% dplyr::select(-1) %>% as.matrix
cv<-cv.glmnet(x,y)
lasso<-glmnet(x,y,lambda=cv$lambda.1se)

coef(lasso)
```
Wow! Crazy, looks like the best predictors of whether a state is Democratic or Republican are opioid prescriptions per thousand (OPPT) and antibiotic prescriptions per thousand (APPT) population. Makes sense, because these are the variables that have been significant the whole time.

####_Model with LASSO Variables_
  
```{r}
part6<-glm(presidential~OPPT+APPT, data=datafinal, family=binomial(link="logit"))
coeftest(part6)
```
Interestingly, the APPT is no longer significant.
  
Okay, now for cross-validation again. Are you ready? Too bad, here it is:  
```{r}
set.seed(1234)
k=10 

data1<-datafinal[sample(nrow(datafinal)),]
folds<-cut(seq(1:nrow(datafinal)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
 train<-data1[folds!=i,]
 test<-data1[folds==i,]
 truth<-test$presidential
 fit<-glm(presidential~OPPT+APPT, data=train, family=binomial(link="logit"))
 probs<-predict(fit,newdata = test,type="response")
 diags<-rbind(diags,class_diag(probs,truth))
}

apply(diags,2,mean)
```
The out-of-sample accuracy is  higher than it was for the LASSO regression model than the original, but not by much! The accuracy for the LASSO model is 0.820, while the original had an accuracy of 0.800.